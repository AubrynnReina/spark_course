{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD is a collection of elements partitioned across nodes\n",
    "- RDD can be persisted in memory\n",
    "- RDD can recover from node failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Parallelized  Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    distData = sc.parallelize(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the number of partitions, we can do it like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    distData = sc.parallelize(data, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All of Spark file-based input methods support:\n",
    "    - Directories\n",
    "    - Compressed files\n",
    "    - Wildcards\n",
    "    \n",
    "- `textFile()` method also has an optional argument for controlling the number of partitions of the file (128MB by HDFS default)\n",
    "\n",
    "- `wholeTextFiles()` and `textFile()` are different in outputs when reading a directory:\n",
    "    - `wholeTextFiles()`: return (key, value) pairs of (filename, content)\n",
    "    - `textFile()`: return lines in each file, no filename specification\n",
    "\n",
    "- RDD can be saved in pickled Python objects: `saveAsPickleFile()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 types of operations:\n",
    "    - transformation: create a new dataset from an existing one\n",
    "    - action: return a value to driver program after running a computation on the dataset\n",
    "\n",
    "- All transformations in Spark are *lazy*, results are not computed right away, they are remembered.\n",
    "\n",
    "- Transformations are computed when an actions required a result to be returned to the drive program\n",
    "\n",
    "- Can use caching for faster access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    lines = sc.textFile(\"data.txt\")\n",
    "    lineLengths = lines.map(lambda s: len(s))\n",
    "    totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first and second lines are not executed right away, not until the third line, which is an action\n",
    "\n",
    "- At that point, Spark breaks the computation into tasks to run on separate machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `lineLengths` again later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # persist() is more customized than cache()\n",
    "    # Allow choosing either RAM or Disks as memory\n",
    "    lineLengths.persist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using a global parameter for keeping track of the progress of a Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just use Accumulators, don't think other solutions just yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Printing elements in an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the RDD to the driver first, either using `collect()` or `take()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Transformation** | **Meaning** |\n",
    "|--|--|\n",
    "| **map**(*func*) | Return a new distributed dataset by passing each element of the source through the *func*|\n",
    "| **filter**(*func*) | Return a new dataset by selecting the elements of the source on which *func* returns True |\n",
    "| **flatMap**(*func*) | Like **map**(*func*) but return a sequence, rather than a single item |\n",
    "| **mapPartitions**(*func*) | Like **map**(*func*) but the *func* should be Iterator &rarr; Iterator, since this transformation runs on partitions |\n",
    "| **mapPartitionsWithIndex**(*func*) | Like **mapPartitions**(*func*) but the *func* should have another parameter to specify which index of the partition to transform |\n",
    "| **sample**(*withReplacement*, *fraction*, *seed*) | Return a sample of the RDD |\n",
    "| **union**(*otherDataset*) | Return a union of the source RDD and the one in the parameter |\n",
    "| **intersection**(*otherDataset*) | Return a interection of the source RDD and the one in the parameter |\n",
    "| **distinct**([*numPartitions*]) | Return a new dataset that contains the distinct elements of the source dataset |\n",
    "| **groupByKey**([*numPartitions*]) | Take in (key, value), return (key, values) in groups based on key |\n",
    "| **reduceByKey**(*func*, [*numPartitions*]) | Take in (key, value), return (key, agg(values)) in groups based on key |\n",
    "| **aggregateByKey**(*func*, [*numPartitions*]) | Take in (key, value), return (key, agg(values)) in groups based on key |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
